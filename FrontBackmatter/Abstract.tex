%*******************************************************
% Abstract
%*******************************************************
%\renewcommand{\abstractname}{Abstract}
\pdfbookmark[1]{Abstract}{Abstract}
% \addcontentsline{toc}{chapter}{\tocEntry{Abstract}}
\begingroup
\let\clearpage\relax
\let\cleardoublepage\relax
\let\cleardoublepage\relax

\chapter*{Summary}
The necessity for \emph{computer-based simulations} is shared by many fields of physics. Specifically, in \emph{High Energy Physics} this necessity is of foremost importance, due to the complexity of the experiments and the vast amount of experimental data which are to be compared to theoretical models. However, starting from the physical calculations, the interaction with the detectors and the reconstruction of physical objects has proven to be extremely \emph{computationally expensive}. Because of this, current collaborations, such as the \emph{Compact Muon Solenoid} (CMS) one, are partially limited by the \emph{quantity} of simulated data available to them, as well by the available \emph{resources} to compute them in a reasonable time. As it has already been the case in countless applications, novel \emph{Machine Learning} (ML) techniques are expected to provide us with the much needed speed and accuracy, an expectation that we thoroughly investigated in the present work, at least for what concerns event simulation. The primary concern of this Thesis has been trying to build a prototype \emph{end-to-end sample analysis} generator, named \emph{FlashSim}.
The classical \emph{FullSim} approach starts from \emph{generator-level} particle listing; propagates stable particles through the detector and emulates the sensor and electronics response producing an output with the same level of detail and the same data representation as the actual detector. The FullSim output is then \emph{reconstructed} with the very same algorithms used on experimental data, calibrated in the same manner and finally transformed in a reduced format usable for the analysis--attempting to capture the changes in the properties of the originally simulated event. Working this way, we thus need to simulate $\approx 1$ MB of detector-like data per event, and then reduce it with the same algorithms used for actual data by a factor of 1000 (1 kB/ev) for analysis.
The key idea behind our work is that an accurate and fast prediction of the final reduction (1 kB/ev) can be achieved starting from the generator-level information alone through a ML approach skipping all intermediate steps.
As a first proof-of-concept, we simulated two classes of physical objects: \emph{jets} and \emph{muons}. This allowed us to put our results to the test in a real-world use case such as the preliminary analysis selection for VBF Channel of H$\rightarrow\mu^+\mu^-$, which mainly relies on those two types of objects.


The results are indeed confirming and possibly exceeding our initial expectation. Through the powerful ML technique of \emph{Normalizing Flows}, we simultaneously generate 22 key reconstructed variables for muons and 17 for jets, starting from \emph{random noise} and \emph{generator-level} physical inputs about the underlying process. The results are compared to the corresponding FullSim simulations results, based on the popular \texttt{Geant4} framework, showing optimal accuracy and preserving all the \emph{correlations} between variables pairs. 
The capacity of our approach to vary its outputs according to the specified physical content of an event is compared to the other major competing approach for fast simulation and it is found to be vastly superior. The proposed approach additionally demonstrates a raw generation speed of \emph{six orders of magnitude} greater than that of the FullSim approach, outputting events at a rate of 33,300 Hz instead of 1 event per minute. After introducing the preprocessing and postprocessing steps needed for a full end-to-end FlashSim generator, we apply it to a complete dataset consisting of different, previously unseen physical processes and we produce a full dataset ready to be used in the VBF H$\rightarrow\mu^+\mu^-$ analysis. We repeat the preliminary analysis selection performed by CMS in 2018, observing good agreement between selected-objects distributions. Additionally, when evaluating the actual \emph{Deep Neural Networks} used in the paper to perform the final signal fit, we obtain compatible outputs between our approach and FullSim, proving that the proposed approach can in fact be employed in a real-case scenario with a fraction of the time and the resources.
The current findings have the potential to completely change the approach to simulations at CMS and at the LHC, paving the way for online, on-demand generation of events. All these results point to interesting and rewarding directions for future research at the boundaries of high energy physics and machine learning.

This Thesis is structured into three parts:

Part 1 presents the context for our work, with Chapter 1 giving an overview of LHC, the CMS Experiment and its physics searches, focusing on the VBF Channel of H$\rightarrow\mu^+\mu^-$, later used as a realistic benchmark. Chapter 2 discusses the current approach to simulation, its costs and main limitations as well as presenting the standard CMS analysis format, the \emph{NanoAOD}.

Part 2 explains the ML tools employed as the backbone of our work, first in a broad and general introduction in Chapter 3 and then with a focus on Normalizing Flows during Chapter 4.

Part 3 presents the main, original contributions, discussing the implementation and the results in Chapter 5, showing the real analysis use case comparison in Chapter 6 and expanding on the conclusion and future outlook in Chapter 7. 
\endgroup

\vfill
