%************************************************
\chapter{Normalizing Flows}\label{ch:mathtest} % $\mathbb{ZNR}$
%************************************************

In standard probabilistic modeling practice, we represent our beliefs over unknown continuous quantities with simple parametric distributions like the normal, exponential, and Laplacian distributions. However, using such simple forms, which are commonly symmetric and unimodal (or have a fixed number of modes when we take a mixture of them), restricts the performance and flexibility of our methods. For instance, standard variational inference in the Variational Autoencoder uses independent univariate normal distributions to represent the variational family. The true posterior is neither independent nor normally distributed, which results in suboptimal inference and simplifies the model that is learnt. Even if we are able to model arbitrary 1-d distributions, with heavy or light tails or any other feature widespread in HEP, we often struggle with \emph{multidimensional} ones.

\emph{Normalizing Flows} are a type of \emph{generative model}, capable of producing new, original samples from a \emph{latent space}, usually a Gaussian one. The main advantage of this approach, when compared to the ones presented in the previous chapter, is that it has been specifically engineered to define explicit \emph{densities}--making it particularly well suited to our use case.

The present chapter serves as a general explanation of the basic concepts and ideas for defining and implementing Normalizing Flows (NF for short). We also briefly discuss in a more general way the possible use cases of this architecture. For the interested reader, an excellent review is the one by Papamakarios et al \cite{papanf}.

\section{Definitions}

Normalizing flows are a family of methods for constructing flexible learnable probability distributions, often with neural networks, which allow us to surpass the limitations of simple parametric forms to represent high-dimensional distributions. Our specific goal is to represent, in fact \emph{learn from data}, the underlying $p^*_x(\mathbf{x})$ for our FullSim simulated samples $\mathbf{x}$ with this formalism, so that we may then draw new, original samples from it.

\subsection{Basics}

The basic idea is to define a multidimensional distribution $p_x(\mathbf{x})$ by passing \emph{random variables} $\mathbf{z} \in \mathbb{R}^D$ drawn from a simple \emph{base distribution} $p_z(\mathbf{z})$, typically a multi-dimensional Gaussian, through a non-linear, \emph{invertible} and \emph{differentiable} transformation \emph{f}: $\mathbb{R}^D \rightarrow \mathbb{R}^D$, $\mathbf{x} = f(\mathbf{z})$. \emph{f} can also be called a \emph{bijection}.

Because $\mathbf{x} = f(\mathbf{z})$, $p_x(\mathbf{x})$ can be expressed as:

\[
p_x(\mathbf{x}) = p_z(\mathbf{z})\det\left|\frac{d\mathbf{z}}{d\mathbf{x}}\right|
\]

Intuitively, if we compose such bijective transformations,\\ $f = (f_{(1)}\circ f_{(2)}\circ\cdots\circ f_{(K)})$, we obtain more \emph{expressive} transforms, thus being able to produce very complicated distributions. In order to do so, we would like to factorize the contribution of each single $f_i$.
Remembering that \emph{f} is invertible, taking the logarithm of both sides we get:

\begin{equation}\label{eqn:logpdf}
	\begin{aligned}
		\log(p_x(x)) &= \log(p_z(f^{-1}(\mathbf{x})))+\log\left(\det\left|\frac{d\mathbf{z}}{d\mathbf{x}}\right|\right)\\
		&= \log(p_z(f^{-1}(\mathbf{x})))-\log\left(\det\left|\frac{d\mathbf{x}}{d\mathbf{z}}\right|\right)
	\end{aligned}
\end{equation}

where $d\mathbf{z}/d\mathbf{x}$ denotes the Jacobian matrix of $f^{-1}(\mathbf{x})$, $\mathbb{J}_{f^{-1}}(\mathbf{x})$.
Intuitively, this equation says that the density of $x$ is equal to the density at the corresponding point in $z$ plus a term that corrects for the warp in volume around an infinitesimally small volume around $x$ caused by the transformation.
 It is clear that, if we have $K$ transforms $f_{(1)}, f_{(2)},\ldots,f_{(K)}$, then the log-density of the transformed variable $\mathbf{x}=(f_{(1)}\circ f_{(2)}\circ\cdots\circ f_{(K)})(\mathbf{z})$ is:
	
	\begin{equation*}
		\begin{aligned}
			\log(p_x(\mathbf{x})) &= \log\left(p_z\left(\left(f_{(K)}^{-1}\circ\cdots\circ f_{(1)}^{-1}\right)\left(\mathbf{x}\right)\right)\right)+\\
			&+\sum^{K}_{k=1}\log\left(\left|\frac{df^{-1}_{(k)}(\mathbf{x}_{(k)})}{d\mathbf{x}'}\right|\right)
		\end{aligned}
	\end{equation*}
	
This relationship between the base distribution and the transformed one through this chain of invertible transforms is at the core of the NF approach and is illustrated in Figure \ref{fig:nf}.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{gfx/ch4/normalizing-flow.png}
    \caption[Normalizing Flows]{The main idea behind Normalizing Flows: how can we find a chain of invertible transformations to send $p_z(\mathbf{z})$ to $p_x(\mathbf{x})$? Taken from \cite{nffig}.}
    \label{fig:nf}
\end{figure}
	
Based on this, the idea is to define some kind of measure, which can then be used as the objective function to minimize, to learn the optimal transformation \emph{f} for defining the target $p_x(\mathbf{x})$, so that sampling becomes as easy as sampling a multi-dimensional Gaussian.

\subsection{Loss functions}

As the idea is to leverage deep learning, we let our transformation \emph{f} depend on a set of \emph{learnable} parameters $\phi$, $f = f(\mathbf{x};\, \phi)$. It should be noted that $\phi$ can actually be a \emph{learnable function} of some parameters, rather than a constant. This will enable us to define conditional pdfs as we will see in the following.
For the sake of completeness, we distinguish two main cases.

\paragraph{Forward KL Divergence}
In our HEP case, we have with billions of available Monte Carlo data and no analytical pdf $p_x^*(\mathbf{x})$. Then, we may define as our loss function the \emph{forward Kullback-Leibler divergence} between the target distribution $p_x^*(\mathbf{x})$ and the flow-defined one $p_x(\mathbf{x}; \, \phi)$:

\begin{equation}\label{eqn:kldiv}
    \begin{aligned}
    \mathcal{L}(\phi) &= \mathcal{D}_{KL}[p_x^*(\mathbf{x})||p_x(\mathbf{x}; \, \phi)]\\
    &= -\mathbb{E}_{p_x^*(\mathbf{x})}[\log(p_x(\mathbf{x}; \, \phi))] +\; \text{const.}\\
    &= -\mathbb{E}_{p_x^*(\mathbf{x})}[\log(p_z(f^{-1}(\mathbf{x}; \, \phi)))+\log\left(\det\mathbb{J}_{f^{-1}}(\mathbf{x}; \, \phi)\right)] +\; \text{const.}
    \end{aligned}
\end{equation}

where we have used Eq. \ref{eqn:logpdf}. Supposing we had a set of training samples $\{\mathbf{x}\}^N_n$ from the target pdf, we may estimate the expectation value over $p_x^*(\mathbf{x})$ by Monte Carlo as:

\[
\mathcal{L}(\phi) \approx -\frac{1}{N} \sum_n [\log(p_z(f^{-1}(\mathbf{x}; \, \phi)))+\log\left(\det\mathbb{J}_{f^{-1}}(\mathbf{x}_n; \, \phi)\right)] +\; \text{const.}
\]

For computing this loss we need to calculate $f^{-1}$, its Jacobian determinant and the density $p_z(f^{-1}(\mathbf{x}; \, \phi))$. 

\paragraph{Reverse KL Divergence} If instead we have the ability to easily evaluate $p_x^*(\mathbf{x})$ but not to sample from it, the \emph{reverse Kullback-Leibler divergence} is more well suited:

\[
\begin{aligned}
    \mathcal{L}(\phi) &= \mathbb{E}_{p_x(\mathbf{x}, \, \phi)}[\log(p_x(\mathbf{x}; \, \phi)) - \log(p_x^*(\mathbf{x}))]\\
    &= \mathbb{E}_{p_x(\mathbf{x}, \, \phi)}[\log(p_z(\mathbf{z}))-\log\left(\det\mathbb{J}_{f}(\mathbf{x}; \, \phi)\right)-\log(p_x(\mathbf{x}; \, \phi))]
\end{aligned}
\]

\section{Constructing flows}
For an actual implementation, the main challenge is in designing parametrizable multivariate bijections that have closed form expressions for both $f$ and $f^{-1}$, a tractable Jacobian, and have enough capacity to model our target pdfs.

\subsection{Coupling Layers}\label{sec:couplay}

One simple way to reduce the computational complexity of the Jacobian is to introduce \emph{coupling layers}. The basic idea is illustrated in Figure \ref{fig:coupla}.

\begin{figure}
    \centering
    \scalebox{0.65}{
    \begin{tikzpicture}[
        thick, node distance=15mm,
        set/.style={draw, diamond, text width=8mm, align=center},
        op/.style={draw, circle, text width=5mm, align=center, fill=orange!40},
      ]
    
      \node[set, fill=blue!20] (z1) {$\vec z_{1:d}$};
      \node[op, right=of z1] (eq) {\raisebox{-1ex}=};
      \node[set, right=of eq, fill=blue!20] (x1) {$\vec x_{1:d}$};
      \draw[->] (z1) edge (eq) (eq) edge (x1);
    
      \node[set, below=1 of z1, fill=green!30] (z2) {$\mathclap{\vec z_{d+1:D}}$};
      \node[op, right=of z2] (g) {$f$};
      \node[below=1em of g] (forward) {forward pass};
      \node[set, right=of g, fill=yellow!40] (x2) {$\mathclap{\vec x_{d+1:D}}$};
      \draw[->] (z2) edge (g) (g) edge (x2);
    
      \node[op] (m) at ($(z1)!0.5!(g)$) {$\phi$};
      \draw[->] (z1) edge (m) (m) edge (g);
    
      \begin{scope}[xshift=9cm]
    
        \node[set, fill=blue!20] (z1) {$\vec z_{1:d}$};
        \node[op, right=of z1] (eq) {\raisebox{-1ex}=};
        \node[set, right=of eq, fill=blue!20] (x1) {$\vec x_{1:d}$};
        \draw[<-] (z1) edge (eq) (eq) edge (x1);
    
        \node[set, below=1 of z1, fill=green!30] (z2) {$\mathclap{\vec z_{d+1:D}}$};
        \node[op, right=of z2] (g) {$\mathclap{f^{-1}}$};
        \node[below=1em of g] (inverse) {inverse pass};
        \node[set, right=of g, fill=yellow!40] (x2) {$\mathclap{\vec x_{d+1:D}}$};
        \draw[<-] (z2) edge (g) (g) edge (x2);
    
        \node[op] (m) at ($(x1)!0.5!(g)$) {$\phi$};
        \draw[->] (x1) edge (m) (m) edge (g);
    
      \end{scope}
    
    \end{tikzpicture}}
    \caption[Coupling layer]{The two possible passes through a coupling layer.}
    \label{fig:coupla}
\end{figure}

We partition the input $\mathbf{z} \in \mathbb{R}^D$ \emph{at any step in the network}, being it the original Gaussian input or the output of some $f_{i-1}$, into two subsets $(\vec z_{1:d}, \, \vec z_{d+1:D}) \in \mathbb{R}^d \times \mathbb{R}^{D-d}$, where \emph{d} is an integer between 1 and \emph{D}, and is usually taken as $\lceil D/2 \rceil$. Then, at each step, the transformation $f_i$ is defined as:

\[
f_i(\mathbf{z}; \, \phi) = f_i(\vec z_{d+1:D}; \, \phi(\vec z_{1:d}))
\]

that is, the single transformation acts only on a \emph{subset} of the input, keeping the other part unchaged and using it as \emph{conditioning} for the actual parameters. 

At the end, the two subset are joined together to form the input for the next layer; by applying permutations between the indexes we can ensure that eventually all values of the input get transformed. Specifically we point out that if we simply apply \emph{linear} permutations and we chain a number of transformations equal to the input dimension, eventually each variable will be transformed based on the conditioning of all the other ones at some point in the network. This means that the total transform will learn to generate final values which are correclty \emph{correlated} to those for the others.
But the greatest advantage is that now the single transformation depends only on one subset of inputs, and thus its jacoban is \emph{block triangular}:

\[
\mathbb{J}_{f}(\mathbf{x}; \, \phi) = 
\begin{pmatrix}
\frac{\partial \vec x_{1:d}}{\partial \vec z_{1:d}} & \frac{\partial \vec x_{1:d}}{\partial \vec z_{d+1:D}}\\
\frac{\partial \vec x_{d+1:D}}{\partial \vec z_{1:d}} & \frac{\partial \vec x_{d+1:D}}{\partial \vec z_{d+1:D}}
\end{pmatrix}
=
\begin{pmatrix}
\mathbb{I} & 0\\
A & \mathbb{J}^*
\end{pmatrix}
\]

The full Jacobian determinant can simply be calculated from the product of the diagonal elements of $\mathbb{J}^*$, which are simply the partial derivatives over ($d+1, \dots D$). A similar reasoning holds for the Jacobian of the inverse.
This significantly speeds up the calculations.

\subsection{Splines}

So far, we did not define the family of functions to use as our $ f_i(\vec z_{d+1:D}; \, \phi(\vec z_{1:d}))$.
There are many possible bijections which one can use in building a NF. Recent advancements have demonstrated the suitability of \emph{rational-quadratic spline transforms} (see \cite{durkan}). 

A monotonic spline is a piecewise function consisting of K segments, where each segment is a simple function that is easy to invert. Specifically, given a set of K+1 input locations $l_{0}, \dots, l_K$ , the transformation
is taken to be a simple monotonic function (e.g. a low-degree polynomial) in each interval
[$l_{k}, l_{k+1}$], under the constraint that the K segments meet at the endpoints.
Outside the interval [$l_{0}, l_K$], the transformer can default to a simple function such as the
identity. Typically, the parameters $\phi$ of the transformations are the input locations, 
the corresponding output locations and (depending on the type of spline) the
derivatives (i.e. slopes) at $l_{0}, \dots, l_K$. An example is illustrated in Figure \ref{fig:rqs}.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{gfx/ch4/D9F0PDyWsAAWKHf.png}
    \caption[Rational quadratic spline]{A rational quadratic spline $g_{\theta}(x)$ and its derivative $g_{\theta}^{'}(x)$}
    \label{fig:rqs}
\end{figure}

Spline-based transformers are as fast to invert as to evaluate, while
maintaining exact analytical invertibility. Evaluating or inverting a spline-based transformer
is done by first locating the right segment--which can be done in $\mathcal{O}$(log K) time using binary
search—and then evaluating or inverting that segment, which is assumed to be analytically
tractable. By increasing the number of segments K, a spline-based transformer can be
made arbitrarily flexible.

\subsection{Conditional distributions}

The theory of Normalizing Flows is also easily generalized to conditional distributions. We denote the variable to condition on by $C=\mathbf{c}\in\mathbb{R}^M$. A simple multivariate source of noise, for example a standard i.i.d. normal distribution, $\mathbf{z}\sim\mathcal{N}(\mathbf{0},I_{D\times D})$, is passed through a vector-valued bijection that also conditions on C, $f:\mathbb{R}^D\times\mathbb{R}^M\rightarrow\mathbb{R}^D$, to produce the more complex transformed variable $\mathbf{x}=f(\mathbf{z};\, \phi(C))$. 

In practice, this is usually accomplished by making the parameters for a known normalizing flow bijection $f$ the output of a neural network that inputs $\mathbf{c}$ as well as one of the subsets of the coupling layer. It is thus straightforward to condition event generation on some ground truth, e.g. the Monte Carlo Gen values matched to our targets. For example, the reconstructed transverse momentum $p_T$ for a jet is expected to be positively correlated to the Gen $p_T$, as are its mass and other key quantities; so we pass the Gen values as inputs to $\phi$.


\section{Applications}\label{sec:nfapp}

The use cases of Normalzing Flows are numerous. 
Here we simply limit ourselves to those that we deem more interesting from the point of view a physicist.

\paragraph{Density estimation and generation}

Due to their ability to be expressive while still allowing for exact likelihood calculations, Normalizing Flows are often used for probabilistic modeling of data. For this application, we assume access to a finite number of draws $\mathbf{x}$ from some unknown generative process $p^*_x(\mathbf{x})$. We then train the Flow according to Eq. \ref{eqn:kldiv}: we can now use the model to estimate densities, expectations, marginals, or other quantities of interest on never-before-seen data; as well as sampling from the model novel instances that could have plausibly been sampled from $p^*_x(\mathbf{x})$. The latter use case, generation, has been a popular application of flows in machine learning, for various categories of data. It is also the one upon which the present work is based.

\paragraph{Inference}
We can also turn to \emph{inference}: estimating unknown quantities within a model. The most common setting is the computation of high-dimensional, analytically intractable integrals of the form: 

\[
\mathcal{I} = \int \text{d}\eta \pi(\eta)
\]

as, for example, the posterior’s normalizing constant or when computing expectations under the posterior in Bayesian inference. Common approaches to this problem include \emph{importance sampling}, where integrals are converted to an expectation under an auxiliary distribution $q(\eta)$:

\[
\mathcal{I} = \int \text{d}\eta \pi(\eta) = \int \text{d}\eta q(\eta) \frac{\pi(\eta)}{q(\eta)} \approx \frac{1}{S}\sum_s \frac{\pi(\eta_s)}{q(\eta_s)}
\]

where $q(\eta)$ is a user-specified density function and $\eta_s$ is a sample from $q(\eta)$. Clearly, this requires both sampling and density evaluation. Since both operations are tractable for many
flows, they make for an attractive model from which to construct the proposal.
Additionally, Flows may also be employed as part of \emph{rejection sampling}, \emph{Markov chain Monte Carlo} and \emph{Variational Inference}, to fit distributions over latent variables or model parameters.

\paragraph{Anomaly detection}

Finally, one last application, which proved successful in the recent LHC Olympics \cite{Kasieczka_2021}, is that of \emph{anomaly detection}, aimed at revealing anomalous feature of data signalling potentially new and insightful physical processes.

The best performance on the first black box in the LHCO challenge, as measured by finding and correctly characterising the anomalous signals, was by a team of cosmologists at Berkeley (George Stein, Uros Seljak and Biwei Dai) who compared the phase-space density between a sliding signal region and sidebands using Flows to model the densities directly from data.

%*****************************************
%*****************************************
%*****************************************
%*****************************************
%*****************************************
